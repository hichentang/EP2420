{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize, scale, minmax_scale, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(data_X, data_Y, number_X, number_Y):\n",
    "    '''\n",
    "    Basic idea:\n",
    "    create several sub-dataframes and concat them together.\n",
    "    For instance:\n",
    "    Samples : 1,2,3,4,5,6,7\n",
    "    number X = 3\n",
    "    number Y = 2\n",
    "    \n",
    "    for measurements, create data frames:\n",
    "    1    |    2    |    3\n",
    "    2    |    3    |    4\n",
    "    3    |    4    |    5\n",
    "    \n",
    "    Then concat them together, we can get 3 new sample measurements:\n",
    "    \n",
    "    Sample 1: 1, 2, 3\n",
    "    Sample 2: 2, 3, 4\n",
    "    Sample 3: 3, 4, 5\n",
    "    '''\n",
    "    \n",
    "    # to get the coloumn names\n",
    "    features_name = [col for col in data_X]\n",
    "    target_name = [col for col in data_Y]\n",
    "    X_shape = data_X.shape\n",
    "    index_X = data_X.index\n",
    "    number_samples = X_shape[0]\n",
    "    \n",
    "    number_to_delete = number_X + number_Y -1\n",
    "    number_measurements = number_samples - number_to_delete\n",
    "    temp_X = []\n",
    "    rst_df = None\n",
    "    rst_index = None\n",
    "    # create number_X dataframes and concat them together\n",
    "    # for each dataframe, we should delete number_X -1 entries\n",
    "    # create the delete lish\n",
    "    for i in range(number_X):\n",
    "        data_copy = data_X.copy()\n",
    "        '''\n",
    "        del_list contains the index of samples which should be deleted for each sub-dataframes\n",
    "        '''\n",
    "        del_list = []\n",
    "        \n",
    "        for j in range(i):\n",
    "            del_list.append(index_X[j])\n",
    "        for k in range(number_to_delete - i):\n",
    "            del_list.append(index_X[number_samples - k -1])\n",
    "        if i == 0:\n",
    "            # the first sub-dataframe, create it directly\n",
    "            rst_df = data_copy.drop(del_list, axis=0)\n",
    "            rst_df.columns = [j+str(i) for j in features_name]\n",
    "            rst_index = rst_df.index\n",
    "        else:\n",
    "            # concat the new sub-dataframe with the previous result\n",
    "#             print(del_list)\n",
    "            temp_df = data_copy.drop(del_list, axis=0)\n",
    "            temp_df.index = rst_index\n",
    "            temp_df.columns = [j+str(i) for j in features_name]\n",
    "            rst_df = pd.concat([rst_df, temp_df], axis=1)\n",
    "#             print(rst_df.shape)\n",
    "            \n",
    "    for i in range(number_Y):\n",
    "        target_copy = data_Y.copy()\n",
    "        del_list = []\n",
    "        for j in range(number_X + i):\n",
    "            del_list.append(index_X[j])\n",
    "        for k in range(number_Y -1 - i):\n",
    "            del_list.append(index_X[number_samples - k -1])\n",
    "#         print(del_list)\n",
    "        if i == 0:\n",
    "            tgt_df = target_copy.drop(del_list, axis=0)\n",
    "            tgt_df.columns = [j+str(i) for j in target_name]\n",
    "            tgt_index = tgt_df.index\n",
    "        else:\n",
    "#             print(del_list)\n",
    "            temp_df = target_copy.drop(del_list, axis=0)\n",
    "            temp_df.index = tgt_index\n",
    "            temp_df.columns = [j+str(i) for j in target_name]\n",
    "            tgt_df = pd.concat([tgt_df, temp_df], axis=1)\n",
    "            print(tgt_df.shape)\n",
    "    return rst_df, tgt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data should be np array, only outliers in X will be considered\n",
    "def OutlierRemoval(InputX, InputY, Threhold):\n",
    "    \n",
    "    Y_FeatureColumn = [i for i in range(len(InputX[0]), len(InputX[0]) + len(InputY[0]))]\n",
    "    \n",
    "    # combine X and Y\n",
    "    combine_data = np.c_[InputX, InputY]\n",
    "    \n",
    "    # remove outlier in X\n",
    "    removed_data = combine_data[np.all(np.abs(np.delete(combine_data, Y_FeatureColumn, 1)) < Threhold ,axis=1)]\n",
    "    \n",
    "    # spilit removed_data into X and Y\n",
    "    removed_x = np.delete(removed_data, Y_FeatureColumn, 1)\n",
    "    removed_y = removed_data[:,Y_FeatureColumn]\n",
    "    \n",
    "    return removed_x, removed_y\n",
    "\n",
    "# All input data should be np array\n",
    "def CalculateNMAE(PredictData, TestData):\n",
    "    return mean_absolute_error(PredictData, TestData)/TestData.mean()\n",
    "\n",
    "\n",
    "# feature selection, only return X since we don't change Y\n",
    "def TreeBasedSelection(InputX, InputY, FeatureNumber):\n",
    "    # create and fit selector\n",
    "    clf = ExtraTreesRegressor()\n",
    "    clf = clf.fit(InputX, InputY)\n",
    "    \n",
    "    # only number of sorted features will be selected, and we disable threshold\n",
    "    model = SelectFromModel(clf, prefit=True, max_features = FeatureNumber, threshold=-np.inf)\n",
    "    OutputX = model.transform(InputX)\n",
    "    return OutputX\n",
    "\n",
    "# standardize column value\n",
    "def ColumnStandardize(Input):\n",
    "    Scaler = StandardScaler()\n",
    "    return Scaler.fit_transform(Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To defince the dataset we use\n",
    "'''\n",
    "# fileName_X = './VoD_Periodic_2017/X.csv'\n",
    "# fileName_Y = './VoD_Periodic_2017/Y.csv'\n",
    "# fileName_X = './KV_flash/X.csv'\n",
    "# fileName_Y = './KV_flash/Y.csv'\n",
    "fileName_X = '../X.csv'\n",
    "fileName_Y = '../Y.csv'\n",
    "# fileName_X = './VoD_flash/X.csv'\n",
    "# fileName_Y = './VoD_flash/Y.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Read data from csv file.\n",
    "X and Y are raw data\n",
    "Y_notime and Y_notime are data without timestamp\n",
    "X_features are a list which contains the name of all of the features in X\n",
    "'''\n",
    "X = pd.read_csv(fileName_X)\n",
    "Y = pd.read_csv(fileName_Y)\n",
    "X_notime = X.drop('TimeStamp',axis=1)\n",
    "Y_notime = Y.drop(['TimeStamp','ReadsAvg'],axis=1)\n",
    "timeIndex_x=pd.to_datetime(X['TimeStamp'])\n",
    "timeIndex_y=pd.to_datetime(Y['TimeStamp'])\n",
    "X.index=timeIndex_x\n",
    "Y.index=timeIndex_y\n",
    "X_notime.index = timeIndex_x\n",
    "Y_notime.index = timeIndex_y\n",
    "X_features = [col for col in X_notime]\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/chen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "# change from dataframe to np array\n",
    "X_npArray = X_notime.to_numpy()\n",
    "Y_npArray = Y_notime.to_numpy()\n",
    "\n",
    "# standardize the data\n",
    "X_Standard = ColumnStandardize(X_npArray)\n",
    "\n",
    "# remove outlier with threshold 100\n",
    "X_NoOutlier, Y_NoOutlier = OutlierRemoval(X_Standard, Y_npArray, 100)\n",
    "\n",
    "# tree based feature selection and have only 16 features\n",
    "X_FeatureSelection = TreeBasedSelection(X_NoOutlier, Y_NoOutlier, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change X from np array back to dataframe\n",
    "\n",
    "feature_name = []\n",
    "for i in range(16):\n",
    "    feature_name.append('feature'+str(i+1))\n",
    "    \n",
    "X_BackToDF = pd.DataFrame(data = X_FeatureSelection,  columns=feature_name)\n",
    "\n",
    "Y_BackToDF = pd.DataFrame(data = Y_NoOutlier,  columns=Y_notime.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set split and sort by index\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_BackToDF, Y_BackToDF, test_size=0.3, random_state=1)\n",
    "X_train = X_train.sort_index(axis = 0)\n",
    "X_test = X_test.sort_index(axis = 0)\n",
    "Y_train = Y_train.sort_index(axis = 0)\n",
    "Y_test = Y_test.sort_index(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13552, 2)\n",
      "(5808, 2)\n",
      "(13551, 2)\n",
      "(5807, 2)\n",
      "(13550, 2)\n",
      "(5806, 2)\n",
      "(13549, 2)\n",
      "(5805, 2)\n",
      "(13548, 2)\n",
      "(5804, 2)\n",
      "(13547, 2)\n",
      "(5803, 2)\n",
      "(13546, 2)\n",
      "(5802, 2)\n",
      "(13545, 2)\n",
      "(5801, 2)\n",
      "(13544, 2)\n",
      "(5800, 2)\n",
      "(13543, 2)\n",
      "(5799, 2)\n",
      "(13542, 2)\n",
      "(5798, 2)\n",
      "(13551, 2)\n",
      "(13551, 3)\n",
      "(5807, 2)\n",
      "(5807, 3)\n",
      "(13550, 2)\n",
      "(13550, 3)\n",
      "(5806, 2)\n",
      "(5806, 3)\n",
      "(13549, 2)\n",
      "(13549, 3)\n",
      "(5805, 2)\n",
      "(5805, 3)\n",
      "(13548, 2)\n",
      "(13548, 3)\n",
      "(5804, 2)\n",
      "(5804, 3)\n",
      "(13547, 2)\n",
      "(13547, 3)\n",
      "(5803, 2)\n",
      "(5803, 3)\n",
      "(13546, 2)\n",
      "(13546, 3)\n",
      "(5802, 2)\n",
      "(5802, 3)\n",
      "(13545, 2)\n",
      "(13545, 3)\n",
      "(5801, 2)\n",
      "(5801, 3)\n",
      "(13544, 2)\n",
      "(13544, 3)\n",
      "(5800, 2)\n",
      "(5800, 3)\n",
      "(13543, 2)\n",
      "(13543, 3)\n",
      "(5799, 2)\n",
      "(5799, 3)\n",
      "(13542, 2)\n",
      "(13542, 3)\n",
      "(5798, 2)\n",
      "(5798, 3)\n",
      "(13541, 2)\n",
      "(13541, 3)\n",
      "(5797, 2)\n",
      "(5797, 3)\n",
      "(13550, 2)\n",
      "(13550, 3)\n",
      "(13550, 4)\n",
      "(5806, 2)\n",
      "(5806, 3)\n",
      "(5806, 4)\n",
      "(13549, 2)\n",
      "(13549, 3)\n",
      "(13549, 4)\n",
      "(5805, 2)\n",
      "(5805, 3)\n",
      "(5805, 4)\n",
      "(13548, 2)\n",
      "(13548, 3)\n",
      "(13548, 4)\n",
      "(5804, 2)\n",
      "(5804, 3)\n",
      "(5804, 4)\n",
      "(13547, 2)\n",
      "(13547, 3)\n",
      "(13547, 4)\n",
      "(5803, 2)\n",
      "(5803, 3)\n",
      "(5803, 4)\n",
      "(13546, 2)\n",
      "(13546, 3)\n",
      "(13546, 4)\n",
      "(5802, 2)\n",
      "(5802, 3)\n",
      "(5802, 4)\n",
      "(13545, 2)\n",
      "(13545, 3)\n",
      "(13545, 4)\n",
      "(5801, 2)\n",
      "(5801, 3)\n",
      "(5801, 4)\n",
      "(13544, 2)\n",
      "(13544, 3)\n",
      "(13544, 4)\n",
      "(5800, 2)\n",
      "(5800, 3)\n",
      "(5800, 4)\n",
      "(13543, 2)\n",
      "(13543, 3)\n",
      "(13543, 4)\n",
      "(5799, 2)\n",
      "(5799, 3)\n",
      "(5799, 4)\n"
     ]
    }
   ],
   "source": [
    "Regressor = LinearRegression()\n",
    "NMAE_table = [[0] * 11] * 11\n",
    "for h in range(1, 12):\n",
    "    for l in range(1, 12):\n",
    "        xtrain, ytrain = create_data(X_train, Y_train, l, h)\n",
    "        xtest, ytest = create_data(X_test, Y_test, l, h)\n",
    "        Linear.fit(xtrain, ytrain)\n",
    "        pred = Linear.predict(xtest)\n",
    "        NMAE_table[h-1][l-1] = CalculateNMAE(pred, ytest)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029760042453995675\n"
     ]
    }
   ],
   "source": [
    "print(CalculateNMAE(pred, ytest)[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain, ytrain = create_data(X_train, Y_train, 1, 1)\n",
    "xtest, ytest = create_data(X_test, Y_test, 1, 1)\n",
    "Linear = LinearRegression()\n",
    "Linear.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Linear.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024153412894430955\n",
      "WritesAvg0    0.024153\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# print(CalculateNMAE(pred[:,1], ytest['WritesAvg1']))\n",
    "print(CalculateNMAE(pred[:,0], ytest['WritesAvg0']))\n",
    "print(CalculateNMAE(pred, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape is (3600, 12)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Data Information\n",
    "'''\n",
    "\n",
    "X_shape = X_notime.shape\n",
    "print(\"X_shape is \" + str(X_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 9, 8]\n",
      "[0, 1, 2, 3, 9]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "#     target_copy = data_Y.copy()\n",
    "    del_list = []\n",
    "    for j in range(3 + i):\n",
    "        del_list.append(j)\n",
    "    for k in range(3 -1 - i):\n",
    "        del_list.append(10 - k -1)\n",
    "    print(del_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
